{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim Word2vec 300D - 201a_gw300_sent.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Download Gensim Model\n",
    "\n",
    "Download a pre-trained word embedding model compatible with Gensim. For example, the Google News Word2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "#gunzip GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Download Dataset\n",
    "\n",
    "Download a sentiment analysis dataset. For example, the airline sentiment dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wget -c \"'https://archive.org/download/misc-dataset/airline-tweets.csv'\"\n",
    "#tar -xvzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Load Gensim Model\n",
    "Load the pre-trained Word2Vec model using Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\zl\\venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\zl\\venv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\zl\\venv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\zl\\venv\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\zl\\venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import gensim as gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load Word2Vec model\n",
    "model_path = r\"C:/ZL/model/GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "print(\"Word2Vec model loaded successfully!\")\n",
    "# 16.6m s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Load Airline Sentiment Dataset\n",
    "Load and explore the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0  570306133677760513           neutral                        1.0000   \n",
      "1  570301130888122368          positive                        0.3486   \n",
      "2  570301083672813571           neutral                        0.6837   \n",
      "3  570301031407624196          negative                        1.0000   \n",
      "4  570300817074462722          negative                        1.0000   \n",
      "\n",
      "  negativereason  negativereason_confidence         airline  \\\n",
      "0            NaN                        NaN  Virgin America   \n",
      "1            NaN                     0.0000  Virgin America   \n",
      "2            NaN                        NaN  Virgin America   \n",
      "3     Bad Flight                     0.7033  Virgin America   \n",
      "4     Can't Tell                     1.0000  Virgin America   \n",
      "\n",
      "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
      "0                    NaN     cairdin                 NaN              0   \n",
      "1                    NaN    jnardino                 NaN              0   \n",
      "2                    NaN  yvonnalynn                 NaN              0   \n",
      "3                    NaN    jnardino                 NaN              0   \n",
      "4                    NaN    jnardino                 NaN              0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0                @VirginAmerica What @dhepburn said.         NaN   \n",
      "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
      "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
      "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
      "\n",
      "               tweet_created tweet_location               user_timezone  \n",
      "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
      "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
      "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
      "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
      "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n",
      "airline_sentiment\n",
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = r\"C:\\ZL\\dataset\\_airsent\\airline-tweets.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Explore data\n",
    "print(data.head())\n",
    "print(data['airline_sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Preprocess\n",
    "Preprocess the text and convert sentiment labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Convert sentiment to numerical labels\n",
    "sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "data['label'] = data['airline_sentiment'].map(sentiment_map)\n",
    "print(\"Preprocessing completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Vectorize\n",
    "Convert text into word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_vector(tokens, model):\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)  # Average of word vectors\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Vectorize dataset\n",
    "data['vector'] = data['tokens'].apply(lambda x: text_to_vector(x, word2vec))\n",
    "X = np.vstack(data['vector'].values)\n",
    "y = data['label'].values\n",
    "print(\"Vectorization completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Train\n",
    "Split the dataset into training and test sets, and train a Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Predict\n",
    "Predict sentiment on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed!\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Prediction completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Evaluate\n",
    "Evaluate the model's performance using common metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.92      0.87      1889\n",
      "           0       0.61      0.46      0.53       580\n",
      "           1       0.76      0.65      0.70       459\n",
      "\n",
      "    accuracy                           0.79      2928\n",
      "   macro avg       0.73      0.68      0.70      2928\n",
      "weighted avg       0.78      0.79      0.78      2928\n",
      "\n",
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved an **accuracy of 79%** on the Airline Sentiment Dataset. \n",
    "\n",
    "Breakdown of the results and observations:\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Observations from the Report**\n",
    "1. **Class-wise Performance**:\n",
    "   - **Negative Sentiment (-1)**: \n",
    "     - **Precision**: 83% (High confidence in predictions for this class).\n",
    "     - **Recall**: 92% (Most negative sentiments were correctly identified).\n",
    "     - **F1-Score**: 87% (Balanced precision and recall for this class).\n",
    "   - **Neutral Sentiment (0)**:\n",
    "     - **Precision**: 61% (Lower confidence in predictions for this class).\n",
    "     - **Recall**: 46% (Many neutral sentiments were missed).\n",
    "     - **F1-Score**: 53% (Moderate performance in this class).\n",
    "   - **Positive Sentiment (1)**:\n",
    "     - **Precision**: 76% (Good confidence in predictions for positive sentiment).\n",
    "     - **Recall**: 65% (Moderate success in identifying positive sentiments).\n",
    "     - **F1-Score**: 70% (Decent balance of precision and recall).\n",
    "\n",
    "2. **Macro Average**:\n",
    "   - Provides an unweighted mean of precision, recall, and F1-score across all classes.\n",
    "   - **F1-Score (70%)**: Indicates that some classes are harder to predict than others (especially the neutral class).\n",
    "\n",
    "3. **Weighted Average**:\n",
    "   - Weights the scores by the number of samples in each class.\n",
    "   - Reflects the overall performance considering the class imbalance.\n",
    "\n",
    "4. **Class Imbalance**:\n",
    "   - The dataset is imbalanced, with significantly more negative samples (1889) compared to neutral (580) and positive (459).\n",
    "   - This likely explains the model's better performance on the negative class and struggles with the neutral class.\n",
    "\n",
    "---\n",
    "\n",
    "### **Suggestions for Improvement**\n",
    "1. **Address Class Imbalance**:\n",
    "   - Use **oversampling** (e.g., SMOTE) for minority classes.\n",
    "   - Apply **class weights** in the Logistic Regression model to balance the loss function.\n",
    "\n",
    "2. **Feature Enhancements**:\n",
    "   - Use **FastText embeddings** to handle out-of-vocabulary (OOV) words better.\n",
    "   - Experiment with contextual embeddings like **BERT** or **RoBERTa** for richer representations.\n",
    "\n",
    "3. **Model Tuning**:\n",
    "   - Experiment with more complex classifiers like Support Vector Machines (SVM) or Random Forests.\n",
    "   - Fine-tune hyperparameters of Logistic Regression (e.g., regularization strength).\n",
    "\n",
    "4. **Add Linguistic Features**:\n",
    "   - Include features like sentiment lexicon scores (e.g., AFINN, SentiWordNet).\n",
    "   - Use bigrams or trigrams in addition to word embeddings.\n",
    "\n",
    "5. **Advanced Preprocessing**:\n",
    "   - Consider stemming or lemmatization.\n",
    "   - Include domain-specific stopwords to clean the data better.\n",
    "\n",
    "By implementing these improvements, you can likely boost the model's performance, especially for the underperforming neutral and positive sentiment classes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
