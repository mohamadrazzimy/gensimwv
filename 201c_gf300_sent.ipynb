{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim Glove 300D - 201a_gg300_sent.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Download Gensim Model\n",
    "\n",
    "Download a pre-trained word embedding model compatible with Gensim. For example, the Google News Word2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "# gunzip cc.en.300.vec.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Download Dataset\n",
    "\n",
    "Download a sentiment analysis dataset. For example, the airline sentiment dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wget -c \"'https://archive.org/download/misc-dataset/airline-tweets.csv'\"\n",
    "#tar -xvzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Load Gensim Model\n",
    "Load the pre-trained model using Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\zl\\venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\zl\\venv\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\zl\\venv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\zl\\venv\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\zl\\venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16196\\2199285001.py:5: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  fasttext_model = FastText.load_fasttext_format(fasttext_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText binary model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Load the FastText binary model\n",
    "fasttext_path = r\"C:/ZL/model/cc.en.300.bin\"\n",
    "fasttext_model = FastText.load_fasttext_format(fasttext_path)\n",
    "print(\"FastText binary model loaded successfully!\")\n",
    "# 4m 28s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Load Airline Sentiment Dataset\n",
    "Load and explore the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0  570306133677760513           neutral                        1.0000   \n",
      "1  570301130888122368          positive                        0.3486   \n",
      "2  570301083672813571           neutral                        0.6837   \n",
      "3  570301031407624196          negative                        1.0000   \n",
      "4  570300817074462722          negative                        1.0000   \n",
      "\n",
      "  negativereason  negativereason_confidence         airline  \\\n",
      "0            NaN                        NaN  Virgin America   \n",
      "1            NaN                     0.0000  Virgin America   \n",
      "2            NaN                        NaN  Virgin America   \n",
      "3     Bad Flight                     0.7033  Virgin America   \n",
      "4     Can't Tell                     1.0000  Virgin America   \n",
      "\n",
      "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
      "0                    NaN     cairdin                 NaN              0   \n",
      "1                    NaN    jnardino                 NaN              0   \n",
      "2                    NaN  yvonnalynn                 NaN              0   \n",
      "3                    NaN    jnardino                 NaN              0   \n",
      "4                    NaN    jnardino                 NaN              0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0                @VirginAmerica What @dhepburn said.         NaN   \n",
      "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
      "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
      "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
      "\n",
      "               tweet_created tweet_location               user_timezone  \n",
      "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
      "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
      "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
      "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
      "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n",
      "airline_sentiment\n",
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = r\"C:\\ZL\\dataset\\_airsent\\airline-tweets.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Explore data\n",
    "print(data.head())\n",
    "print(data['airline_sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Preprocess\n",
    "Preprocess the text and convert sentiment labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['tokens'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Convert sentiment to numerical labels\n",
    "sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "data['label'] = data['airline_sentiment'].map(sentiment_map)\n",
    "print(\"Preprocessing completed!\")\n",
    "# 1m 9s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Vectorize\n",
    "Convert text into word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_fasttext_vector(word, model, vector_size=300):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except KeyError:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "def text_to_vector_fasttext(tokens, model, vector_size=300):\n",
    "    vectors = [get_fasttext_vector(word, model, vector_size) for word in tokens]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)  # Average of word vectors\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Apply vectorization\n",
    "data['vector'] = data['tokens'].apply(lambda x: text_to_vector_fasttext(x, fasttext_model))\n",
    "X = np.vstack(data['vector'].values)\n",
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Train\n",
    "Split the dataset into training and test sets, and train a Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Predict\n",
    "Predict sentiment on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed!\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Prediction completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Evaluate\n",
    "Evaluate the model's performance using common metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.94      0.87      1889\n",
      "           0       0.65      0.44      0.52       580\n",
      "           1       0.81      0.58      0.67       459\n",
      "\n",
      "    accuracy                           0.79      2928\n",
      "   macro avg       0.75      0.65      0.69      2928\n",
      "weighted avg       0.78      0.79      0.77      2928\n",
      "\n",
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FastText-based model achieved an **accuracy of 79%**, which matches the performance of the Word2Vec model. \n",
    "Breakdown of the results and observations:\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Observations from FastText Results**\n",
    "1. **Class-wise Performance**:\n",
    "   - **Negative Sentiment (-1)**: \n",
    "     - **Precision (81%)** and **Recall (94%)** remain strong, similar to Word2Vec and GloVe.\n",
    "     - **F1-Score (87%)** indicates the model's strength in predicting the majority class.\n",
    "   - **Neutral Sentiment (0)**: \n",
    "     - **Precision (65%)** is higher than both Word2Vec (61%) and GloVe (59%).\n",
    "     - **Recall (44%)** remains comparable to the other models, reflecting challenges in capturing neutral sentiment.\n",
    "     - **F1-Score (52%)** is slightly better than GloVe and matches Word2Vec.\n",
    "   - **Positive Sentiment (1)**:\n",
    "       - **Precision (81%)** is the highest among the models (Word2Vec: 76%, GloVe: 75%).\n",
    "       - **Recall (58%)** lags slightly behind Word2Vec (65%) but is better than GloVe (62%).\n",
    "       - **F1-Score (67%)** is comparable to GloVe and slightly lower than Word2Vec (70%).\n",
    "\n",
    "2. **Macro Average**:\n",
    "   - **F1-Score (69%)** is higher than GloVe (68%) but slightly lower than Word2Vec (70%).\n",
    "\n",
    "3. **Weighted Average**:\n",
    "   - **F1-Score (77%)** is comparable to Word2Vec (78%) and slightly better than GloVe (76%).\n",
    "\n",
    "4. **Class Imbalance**:\n",
    "   - FastText handles class imbalance better than GloVe, especially for the neutral and positive classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison: FastText vs. Word2Vec vs. GloVe**\n",
    "| Metric                | Word2Vec | GloVe | FastText |\n",
    "|-----------------------|----------|-------|----------|\n",
    "| Accuracy              | 79%      | 78%   | 79%      |\n",
    "| Negative F1-Score (-1)| 87%      | 87%   | 87%      |\n",
    "| Neutral F1-Score (0)  | 53%      | 50%   | 52%      |\n",
    "| Positive F1-Score (1) | 70%      | 68%   | 67%      |\n",
    "| Macro F1-Score        | 70%      | 68%   | 69%      |\n",
    "| Weighted F1-Score     | 78%      | 76%   | 77%      |\n",
    "\n",
    "---\n",
    "\n",
    "### **Observations**\n",
    "1. **Strengths of FastText**:\n",
    "   - Improved precision for the neutral and positive classes.\n",
    "   - Handles rare and OOV words better due to subword representations.\n",
    "\n",
    "2. **Trade-Offs**:\n",
    "   - Recall for the positive class is slightly lower than Word2Vec.\n",
    "   - Longer loading times compared to GloVe and Word2Vec.\n",
    "\n",
    "3. **Overall**:\n",
    "   - FastText provides comparable accuracy and F1-scores to Word2Vec while outperforming GloVe in most metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### **Suggestions for Further Improvement**\n",
    "1. **Fine-Tune FastText**:\n",
    "   - Fine-tune the embeddings on domain-specific data (e.g., airline reviews).\n",
    "\n",
    "2. **Hybrid Approach**:\n",
    "   - Combine embeddings (e.g., concatenate FastText and Word2Vec) for richer representations.\n",
    "\n",
    "3. **Advanced Models**:\n",
    "   - Use contextual embeddings (e.g., BERT, RoBERTa) for further improvement.\n",
    "\n",
    "4. **Class Imbalance**:\n",
    "   - Apply **oversampling**, **undersampling**, or **class-weighting** to address the imbalance in neutral and positive sentiments.\n",
    "\n",
    "FastText has shown strong performance, particularly for precision in minority classes. With its ability to handle rare words effectively, it remains a competitive option for sentiment analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
